# -*- coding: utf-8 -*-
"""20Nov2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PzSQ7I2DiD7ZNOfL6hMrxycnZ3IZjwLn
"""

# Importing Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
file_path = "HCAHPS_Hospital_2023_1.csv"
data_hc = pd.read_csv(file_path)

# Display the first few rows of the dataset
print("Dataset Preview:")
print(data_hc.head())

# Check dataset structure
print("\nDataset Information:")
print(data_hc.info())

# Display the columns before removal
print("Columns before removal:")
print(data_hc.columns)

# Remove the specified columns
columns_to_remove = [
    'Patient Survey Star Rating Footnote',
    'HCAHPS Answer Percent Footnote',
    'Number of Completed Surveys Footnote',
    'Survey Response Rate Percent Footnote'
]

# Dropping the columns
data_hc = data_hc.drop(columns=columns_to_remove)

# Display the columns after removal
print("\nColumns after removal:")
print(data_hc.columns)

# Check for missing values
print("\nMissing Values Count:")
print(data_hc.isnull().sum())

# Check dataset structure
print("\nDataset Information:")
print(data_hc.info())

# Check the column before conversion
print("Unique values in 'Patient Survey Star Rating' before conversion:")
print(data_hc['Patient Survey Star Rating'].unique())

print("\nUnique values in 'HCAHPS Answer Percent' before conversion:")
print(data_hc['HCAHPS Answer Percent'].unique())

print("\nUnique values in 'HCAHPS Linear Mean Value' before conversion:")
print(data_hc['HCAHPS Linear Mean Value'].unique())

print("\nUnique values in 'Number of Completed Surveys' before conversion:")
print(data_hc['Number of Completed Surveys'].unique())

print("\nUnique values in 'Survey Response Rate Percent' before conversion:")
print(data_hc['Survey Response Rate Percent'].unique())

# Convert to numeric, forcing non-numeric values to NaN
data_hc['Patient Survey Star Rating'] = pd.to_numeric(data_hc['Patient Survey Star Rating'], errors='coerce')
data_hc['HCAHPS Answer Percent'] = pd.to_numeric(data_hc['HCAHPS Answer Percent'], errors='coerce')
data_hc['HCAHPS Linear Mean Value'] = pd.to_numeric(data_hc['HCAHPS Linear Mean Value'], errors='coerce')
data_hc['Number of Completed Surveys'] = pd.to_numeric(data_hc['Number of Completed Surveys'], errors='coerce')
data_hc['Survey Response Rate Percent'] = pd.to_numeric(data_hc['Survey Response Rate Percent'], errors='coerce')

# Check the column after conversion
print("\nUnique values in 'Patient Survey Star Rating' after conversion:")
print(data_hc['Patient Survey Star Rating'].unique())

print("\nUnique values in 'HCAHPS Answer Percent' after conversion:")
print(data_hc['HCAHPS Answer Percent'].unique())

print("\nUnique values in 'HCAHPS Linear Mean Value' after conversion:")
print(data_hc['HCAHPS Linear Mean Value'].unique())

print("\nUnique values in 'Number of Completed Surveys' after conversion:")
print(data_hc['Number of Completed Surveys'].unique())

print("\nUnique values in 'Survey Response Rate Percent' after conversion:")
print(data_hc['Survey Response Rate Percent'].unique())

# Check for null values
print("\nCount of NaN values in 'Patient Survey Star Rating':")
print(data_hc['Patient Survey Star Rating'].isnull().sum())

print("\nCount of NaN values in 'HCAHPS Answer Percent':")
print(data_hc['HCAHPS Answer Percent'].isnull().sum())

print("\nCount of NaN values in 'HCAHPS Linear Mean Value':")
print(data_hc['HCAHPS Linear Mean Value'].isnull().sum())

print("\nCount of NaN values in 'Number of Completed Surveys':")
print(data_hc['Number of Completed Surveys'].isnull().sum())

print("\nCount of NaN values in 'Survey Response Rate Percent':")
print(data_hc['Survey Response Rate Percent'].isnull().sum())

# Summary statistics of numerical features
print("\nSummary Statistics:")
print(data_hc.describe())

# Categorical variables analysis
categorical_columns = data_hc.select_dtypes(include=['object']).columns
for col in categorical_columns:
    print(f"\nValue Counts for {col}:")
    print(data_hc[col].value_counts())

# Define the columns to exclude
exclude_columns = [
    "ZIP Code",
    "HCAHPS Answer Percent",
    "HCAHPS Linear Mean Value",
    "Number of Completed Surveys",
    "Survey Response Rate Percent"
]

# Select numerical columns and exclude the specified ones
numerical_columns = [
    col for col in data_hc.select_dtypes(include=['int64', 'float64']).columns
    if col not in exclude_columns
]

# Plot the distribution for numerical variables
for col in numerical_columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(data_hc[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.show()

# Define the columns to exclude
exclude_columns = [
    "ZIP Code",
    "HCAHPS Answer Percent",
    "HCAHPS Linear Mean Value",
    "Number of Completed Surveys",
    "Survey Response Rate Percent"
]

# Filter numerical_columns to exclude the specified columns
filtered_numerical_columns = [col for col in numerical_columns if col not in exclude_columns]

# Boxplots for numerical features to detect outliers
for col in filtered_numerical_columns:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=data_hc[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

# Define the columns to exclude
exclude_columns = [
    "Facility Name",
    "Address",
    "City/Town",
    "State",
    "ZIP Code",
    "County/Parish",
    "ZIP Code",
    "HCAHPS Answer Percent",
    "Survey Response Rate Percent",
    "Start Date",
    "End Date"
]

# Select only numeric columns and exclude the specified ones
numeric_data = data_hc.select_dtypes(include=['int64', 'float64']).drop(columns=exclude_columns, errors='ignore')

# Check if there are numeric columns left after exclusion
if numeric_data.empty:
    print("No numeric columns available for correlation analysis.")
else:
    # Plot the heatmap for numeric columns
    plt.figure(figsize=(10, 6))
    sns.heatmap(numeric_data.corr(), annot=True, fmt=".2f", cmap="coolwarm")
    plt.title("Correlation Heatmap")
    plt.show()

# Define the columns to exclude
exclude_columns = [
    "Facility ID",
    "Address",
    "City/Town",
    "State",
    "County/Parish",
    "Telephone Number",
    "HCAHPS Measure ID",
    "Start Date",
    "End Date"
]

# Filter categorical columns to exclude the specified ones
filtered_categorical_columns = [col for col in categorical_columns if col not in exclude_columns]

# Bar plots for categorical variables
for col in filtered_categorical_columns:
    plt.figure(figsize=(10, 4))
    sns.countplot(data_hc[col], order=data_hc[col].value_counts().index, palette="viridis")
    plt.title(f"Count Plot of {col}")
    plt.xticks(rotation=45)
    plt.show()

# Define the columns to exclude
exclude_columns = [
    "Facility ID",
    "Address",
    "City/Town",
    "State",
    "ZIP Code",
    "County/Parish",
    "Telephone Number",
    "HCAHPS Measure ID",
    "HCAHPS Linear Mean Value",
    "Start Date",
    "End Date"
]

# Filter numerical columns to exclude the specified ones
numerical_columns = [
    col for col in data_hc.select_dtypes(include=['int64', 'float64']).columns
    if col not in exclude_columns
]

# Analyze relationships (bivariate analysis) for numerical columns
if len(numerical_columns) > 1:
    sns.pairplot(data_hc[numerical_columns], diag_kind="kde")
    plt.suptitle("Pair Plot of Numerical Variables", y=1.02)  # Adjust the title position
    plt.show()
else:
    print("Not enough numerical columns for bivariate analysis.")

# Example: Analyze how a numerical variable varies with a categorical variable
target_column = "Patient Survey Star Rating"  # Replace with a relevant numerical column
categorical_col = "Facility Name"  # Replace with a relevant categorical column

# Check if the target column and categorical column exist in the dataset
if target_column in numerical_columns and categorical_col in categorical_columns:
    # Filter the dataset to remove rows where the target numerical column has null values and values <= 0
    data_filtered = data_hc[(data_hc[target_column].notnull()) & (data_hc[target_column] > 0)]

    # Get the count of rows after filtering
    filtered_row_count = data_filtered.shape[0]
    print(f"Number of rows after filtering: {filtered_row_count}")

    # Check if there are any non-null, valid values left in the filtered dataset
    if filtered_row_count > 0:
        plt.figure(figsize=(12, 6))
        sns.boxplot(x=data_filtered[categorical_col], y=data_filtered[target_column], palette="Set2")
        plt.title(f"{target_column} vs {categorical_col}", fontsize=16)
        plt.xticks(rotation=45)
        plt.show()
    else:
        print(f"No valid values greater than 0 in '{target_column}'.")
else:
    print("No numeric or categorical columns available for analysis.")

# Group the data by 'Facility Name' and count the number of surveys (rows) for each facility
facility_survey_count = data_hc['Facility Name'].value_counts()

# Get the top 10 facilities with the highest survey count
top_10_facilities = facility_survey_count.head(10)

# Display the top 10 facilities
print("Top 10 Facilities with the Highest Survey Counts:")
print(top_10_facilities)

# Optionally, you can visualize this in a bar plot
plt.figure(figsize=(12, 6))
sns.barplot(x=top_10_facilities.index, y=top_10_facilities.values, palette="viridis")
plt.title("Top 10 Facilities with the Highest Survey Counts", fontsize=16)
plt.xticks(rotation=45)
plt.xlabel("Facility Name")
plt.ylabel("Survey Count")
plt.show()

# Group the data by 'Facility Name' and calculate the average survey rating for each facility
facility_avg_rating = data_hc.groupby('Facility Name')['Patient Survey Star Rating'].mean()

# Get the top 10 facilities with the highest average survey rating
top_10_facilities_rating = facility_avg_rating.sort_values(ascending=False).head(10)

# Display the top 10 facilities with the highest average ratings
print("Top 10 Facilities with the Highest Average Survey Ratings:")
print(top_10_facilities_rating)

# Optionally, you can visualize this in a bar plot
plt.figure(figsize=(12, 6))
sns.barplot(x=top_10_facilities_rating.index, y=top_10_facilities_rating.values, palette="viridis")
plt.title("Top 10 Facilities with the Highest Average Survey Ratings", fontsize=16)
plt.xticks(rotation=45)
plt.xlabel("Facility Name")
plt.ylabel("Average Survey Rating")
plt.show()

!pip install vaderSentiment

# Import required libraries
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from nltk import download

# Download necessary NLTK resources
download('punkt')
download('stopwords')
download('wordnet')

# Download the 'punkt_tab' resource
download('punkt_tab')

# Initialize lemmatizer and VADER analyzer
lemmatizer = WordNetLemmatizer()
analyzer = SentimentIntensityAnalyzer()

# Function to clean and preprocess text
def clean_text(text):
    if not isinstance(text, str):
        text = str(text)

    # Step 1: Remove HTML tags and URLs
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'http\S+|www\S+', '', text)  # Remove URLs

    # Step 2: Normalize text
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^a-z\s]', '', text)  # Remove special characters and punctuation

    # Tokenize and remove stopwords
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Perform lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return ' '.join(tokens)

# Function to label sentiment using TextBlob
def label_sentiment_textblob(text):
    polarity = TextBlob(text).sentiment.polarity
    if polarity > 0:
        return 'positive'
    elif polarity == 0:
        return 'neutral'
    else:
        return 'negative'

# Function to label sentiment using VADER with adjustable threshold
def label_sentiment_vader_adjusted(text, neutral_threshold=0.1):
    sentiment_score = analyzer.polarity_scores(text)
    compound_score = sentiment_score['compound']
    if compound_score > neutral_threshold:
        return 'positive'
    elif compound_score < -neutral_threshold:
        return 'negative'
    else:
        return 'neutral'

# Function to refine sentiment based on rules
def refine_sentiment(row):
    text = row['Cleaned_Answer_Description']
    textblob_sentiment = row['TextBlob_Sentiment']
    vader_sentiment = row['VADER_Sentiment']

    # Override if specific terms are present
    if "never" in text:
        return 'negative'

    # Default to VADER if results mismatch
    if textblob_sentiment != vader_sentiment:
        return vader_sentiment
    return textblob_sentiment

# Preprocess the text
data_hc['Cleaned_Answer_Description'] = data_hc['HCAHPS Answer Description'].fillna("").apply(clean_text)

# Apply TextBlob sentiment labeling
data_hc['TextBlob_Sentiment'] = data_hc['Cleaned_Answer_Description'].apply(label_sentiment_textblob)

# Apply VADER sentiment labeling
data_hc['VADER_Sentiment'] = data_hc['Cleaned_Answer_Description'].apply(label_sentiment_vader_adjusted)

# Add polarity scores for analysis
data_hc['TextBlob_Polarity'] = data_hc['Cleaned_Answer_Description'].apply(lambda x: TextBlob(x).sentiment.polarity)
data_hc['VADER_Compound'] = data_hc['Cleaned_Answer_Description'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

# Refine sentiment
data_hc['Final_Sentiment'] = data_hc.apply(refine_sentiment, axis=1)

# Save the final labeled dataset
data_hc.to_csv("sentiment_labeled_data.csv", index=False)

# Print mismatched sentiment examples
mismatched = data_hc[data_hc['TextBlob_Sentiment'] != data_hc['VADER_Sentiment']]
print("Mismatched Sentiments:")
print(mismatched[['Cleaned_Answer_Description', 'TextBlob_Sentiment', 'VADER_Sentiment']])

#Visualize the distribution of sentiments
#Identify common words using word clouds (WordCloud library)
!pip install wordcloud matplotlib seaborn

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns

# Plotting the sentiment distribution (VADER Sentiment)
plt.figure(figsize=(10, 6))
sns.countplot(x='VADER_Sentiment', data=data_hc, palette='Set2')
plt.title('Distribution of Sentiments (VADER)', fontsize=16)
plt.xlabel('Sentiment', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.show()

# Generate a Word Cloud for all cleaned text
all_text = ' '.join(data_hc['Cleaned_Answer_Description'])

# Create a word cloud for all text
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Display the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud for All Text', fontsize=16)
plt.axis('off')
plt.show()

# Generate word clouds based on VADER sentiment labels
positive_text = ' '.join(data_hc[data_hc['VADER_Sentiment'] == 'positive']['Cleaned_Answer_Description'])
negative_text = ' '.join(data_hc[data_hc['VADER_Sentiment'] == 'negative']['Cleaned_Answer_Description'])
neutral_text = ' '.join(data_hc[data_hc['VADER_Sentiment'] == 'neutral']['Cleaned_Answer_Description'])

# Word cloud for positive sentiment
wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.title('Word Cloud for Positive Sentiment (VADER)', fontsize=16)
plt.axis('off')
plt.show()

# Word cloud for negative sentiment
wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative_text)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.title('Word Cloud for Negative Sentiment (VADER)', fontsize=16)
plt.axis('off')
plt.show()

# Word cloud for neutral sentiment
wordcloud_neutral = WordCloud(width=800, height=400, background_color='white').generate(neutral_text)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud_neutral, interpolation='bilinear')
plt.title('Word Cloud for Neutral Sentiment (VADER)', fontsize=16)
plt.axis('off')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split

# Prepare the features and labels
X = data_hc['Cleaned_Answer_Description']  # Features (cleaned text)
y = data_hc['VADER_Sentiment']  # Labels (VADER sentiments)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shape of the training and testing sets
print("Training Data:")
print("X_train:", X_train.shape)
print("y_train:", y_train.shape)

print("\nTesting Data:")
print("X_test:", X_test.shape)
print("y_test:", y_test.shape)

#Convert Text to Numeric Representation
from sklearn.feature_extraction.text import CountVectorizer

# Sample DataFrame
import pandas as pd

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the 'Cleaned_Answer_Description' column to get the document-term matrix
X_bow = vectorizer.fit_transform(data_hc['Cleaned_Answer_Description'])

# Convert the result to a dense matrix and display it
X_bow_dense = X_bow.toarray()

# Show the feature names (words in the vocabulary)
print("Vocabulary:", vectorizer.get_feature_names_out())

# Show the document-term matrix
print("Document-Term Matrix (BoW):\n", X_bow_dense)

#Support Vector Machine (SVM)

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer

# Initialize the TF-IDF vectorizer
vectorizer = TfidfVectorizer()

# Fit the vectorizer to the training data and transform it
X_train_vec = vectorizer.fit_transform(X_train)

# Transform the test data using the fitted vectorizer
X_test_vec = vectorizer.transform(X_test)

# Initialize the SVM model
svm_model = SVC(kernel='linear')

# Train the model using the vectorized training data
svm_model.fit(X_train_vec, y_train)

# Make predictions using the vectorized test data
y_pred_svm = svm_model.predict(X_test_vec)

# Evaluate the model
print("SVM Classification Report:\n", classification_report(y_test, y_pred_svm))

